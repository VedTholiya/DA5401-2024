{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ab_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>...</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>76698</td>\n",
       "      <td>na</td>\n",
       "      <td>2130706438</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1240520</td>\n",
       "      <td>493384</td>\n",
       "      <td>721044</td>\n",
       "      <td>469792</td>\n",
       "      <td>339156</td>\n",
       "      <td>157956</td>\n",
       "      <td>73224</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>33058</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>421400</td>\n",
       "      <td>178064</td>\n",
       "      <td>293306</td>\n",
       "      <td>245416</td>\n",
       "      <td>133654</td>\n",
       "      <td>81140</td>\n",
       "      <td>97576</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>41040</td>\n",
       "      <td>na</td>\n",
       "      <td>228</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>277378</td>\n",
       "      <td>159812</td>\n",
       "      <td>423992</td>\n",
       "      <td>409564</td>\n",
       "      <td>320746</td>\n",
       "      <td>158022</td>\n",
       "      <td>95128</td>\n",
       "      <td>514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>240</td>\n",
       "      <td>46</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>60874</td>\n",
       "      <td>na</td>\n",
       "      <td>1368</td>\n",
       "      <td>458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>622012</td>\n",
       "      <td>229790</td>\n",
       "      <td>405298</td>\n",
       "      <td>347188</td>\n",
       "      <td>286954</td>\n",
       "      <td>311560</td>\n",
       "      <td>433954</td>\n",
       "      <td>1218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  aa_000 ab_000      ac_000 ad_000 ae_000 af_000 ag_000 ag_001 ag_002  \\\n",
       "0   neg   76698     na  2130706438    280      0      0      0      0      0   \n",
       "1   neg   33058     na           0     na      0      0      0      0      0   \n",
       "2   neg   41040     na         228    100      0      0      0      0      0   \n",
       "3   neg      12      0          70     66      0     10      0      0      0   \n",
       "4   neg   60874     na        1368    458      0      0      0      0      0   \n",
       "\n",
       "   ...   ee_002  ee_003  ee_004  ee_005  ee_006  ee_007  ee_008 ee_009 ef_000  \\\n",
       "0  ...  1240520  493384  721044  469792  339156  157956   73224      0      0   \n",
       "1  ...   421400  178064  293306  245416  133654   81140   97576   1500      0   \n",
       "2  ...   277378  159812  423992  409564  320746  158022   95128    514      0   \n",
       "3  ...      240      46      58      44      10       0       0      0      4   \n",
       "4  ...   622012  229790  405298  347188  286954  311560  433954   1218      0   \n",
       "\n",
       "  eg_000  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3     32  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 171 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load the IDA2016 Challenge dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('IDA2016Challenge/aps_failure_training_set.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's replace all the 'na' with 0.\n",
    "df[df == 'na'] = 0\n",
    "df_us = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try some random undersampling.\n",
    "df_pos = df[df['class']=='pos']\n",
    "df_neg = df[df['class']=='neg']\n",
    "df_neg_us = df_neg.sample(10000)\n",
    "df_us = pd.concat((df_pos,df_neg_us))\n",
    "df_us.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims = (60000, 170)\n",
      "Y distrib = class\n",
      "neg    59000\n",
      "pos     1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# extract the features and labels\n",
    "y = df_us['class']\n",
    "X = df_us.drop(columns=['class'])\n",
    "print(\"X dims =\", X.shape)\n",
    "print(\"Y distrib =\", y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'neg' 'neg' ... 'neg' 'neg' 'neg']\n"
     ]
    }
   ],
   "source": [
    "# y.values gets us (N,1) and ravel() gets us (N,) as the shapes.\n",
    "ybin = y.values.ravel()\n",
    "print(ybin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's partition the data into train-test splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, ybin,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(Xtrain.astype(np.float32))\n",
    "X_test_scaled = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build the SVM classifier to get the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 3/5; 1/12] START C=50, gamma=1e-05..........................................\n",
      "[CV 1/5; 1/12] START C=50, gamma=1e-05..........................................\n",
      "[CV 4/5; 1/12] START C=50, gamma=1e-05..........................................\n",
      "[CV 1/5; 2/12] START C=50, gamma=0.0001.........................................\n",
      "[CV 5/5; 1/12] START C=50, gamma=1e-05..........................................\n",
      "[CV 4/5; 2/12] START C=50, gamma=0.0001.........................................\n",
      "[CV 5/5; 2/12] START C=50, gamma=0.0001.........................................\n",
      "[CV 3/5; 3/12] START C=50, gamma=0.001..........................................\n",
      "[CV 2/5; 1/12] START C=50, gamma=1e-05..........................................\n",
      "[CV 3/5; 2/12] START C=50, gamma=0.0001.........................................\n",
      "[CV 4/5; 3/12] START C=50, gamma=0.001..........................................\n",
      "[CV 1/5; 3/12] START C=50, gamma=0.001..........................................\n",
      "[CV 5/5; 3/12] START C=50, gamma=0.001..........................................\n",
      "[CV 2/5; 2/12] START C=50, gamma=0.0001.........................................\n",
      "[CV 2/5; 3/12] START C=50, gamma=0.001..........................................\n",
      "[CV 1/5; 4/12] START C=100, gamma=1e-05.........................................\n",
      "[CV 2/5; 3/12] END ...........C=50, gamma=0.001;, score=0.989 total time=  56.3s\n",
      "[CV 2/5; 4/12] START C=100, gamma=1e-05.........................................\n",
      "[CV 1/5; 3/12] END ...........C=50, gamma=0.001;, score=0.991 total time= 1.0min\n",
      "[CV 3/5; 4/12] START C=100, gamma=1e-05.........................................\n",
      "[CV 3/5; 3/12] END ...........C=50, gamma=0.001;, score=0.988 total time= 1.2min\n",
      "[CV 4/5; 4/12] START C=100, gamma=1e-05.........................................\n",
      "[CV 4/5; 3/12] END ...........C=50, gamma=0.001;, score=0.990 total time= 1.2min\n",
      "[CV 5/5; 4/12] START C=100, gamma=1e-05.........................................\n",
      "[CV 5/5; 3/12] END ...........C=50, gamma=0.001;, score=0.989 total time= 1.2min\n",
      "[CV 1/5; 5/12] START C=100, gamma=0.0001........................................\n",
      "[CV 2/5; 2/12] END ..........C=50, gamma=0.0001;, score=0.985 total time= 1.5min\n",
      "[CV 2/5; 5/12] START C=100, gamma=0.0001........................................\n",
      "[CV 5/5; 2/12] END ..........C=50, gamma=0.0001;, score=0.987 total time= 1.7min\n",
      "[CV 3/5; 5/12] START C=100, gamma=0.0001........................................\n",
      "[CV 1/5; 2/12] END ..........C=50, gamma=0.0001;, score=0.985 total time= 1.7min\n",
      "[CV 4/5; 5/12] START C=100, gamma=0.0001........................................\n",
      "[CV 4/5; 2/12] END ..........C=50, gamma=0.0001;, score=0.986 total time= 1.8min\n",
      "[CV 5/5; 5/12] START C=100, gamma=0.0001........................................\n",
      "[CV 2/5; 1/12] END ...........C=50, gamma=1e-05;, score=0.979 total time= 1.9min\n",
      "[CV 1/5; 6/12] START C=100, gamma=0.001.........................................\n",
      "[CV 5/5; 1/12] END ...........C=50, gamma=1e-05;, score=0.977 total time= 2.0min\n",
      "[CV 2/5; 6/12] START C=100, gamma=0.001.........................................\n",
      "[CV 3/5; 2/12] END ..........C=50, gamma=0.0001;, score=0.985 total time= 2.0min\n",
      "[CV 3/5; 6/12] START C=100, gamma=0.001.........................................\n",
      "[CV 4/5; 1/12] END ...........C=50, gamma=1e-05;, score=0.979 total time= 2.1min\n",
      "[CV 4/5; 6/12] START C=100, gamma=0.001.........................................\n",
      "[CV 1/5; 4/12] END ..........C=100, gamma=1e-05;, score=0.983 total time= 2.2min\n",
      "[CV 5/5; 6/12] START C=100, gamma=0.001.........................................\n",
      "[CV 1/5; 1/12] END ...........C=50, gamma=1e-05;, score=0.983 total time= 2.3min\n",
      "[CV 1/5; 7/12] START C=500, gamma=1e-05.........................................\n",
      "[CV 3/5; 1/12] END ...........C=50, gamma=1e-05;, score=0.981 total time= 2.4min\n",
      "[CV 2/5; 7/12] START C=500, gamma=1e-05.........................................\n",
      "[CV 2/5; 4/12] END ..........C=100, gamma=1e-05;, score=0.980 total time= 1.6min\n",
      "[CV 3/5; 7/12] START C=500, gamma=1e-05.........................................\n",
      "[CV 2/5; 6/12] END ..........C=100, gamma=0.001;, score=0.989 total time=  49.2s\n",
      "[CV 4/5; 7/12] START C=500, gamma=1e-05.........................................\n",
      "[CV 1/5; 6/12] END ..........C=100, gamma=0.001;, score=0.990 total time=  57.6s\n",
      "[CV 5/5; 7/12] START C=500, gamma=1e-05.........................................\n",
      "[CV 2/5; 5/12] END .........C=100, gamma=0.0001;, score=0.987 total time= 1.5min\n",
      "[CV 1/5; 8/12] START C=500, gamma=0.0001........................................\n",
      "[CV 3/5; 6/12] END ..........C=100, gamma=0.001;, score=0.987 total time=  58.4s\n",
      "[CV 2/5; 8/12] START C=500, gamma=0.0001........................................\n",
      "[CV 1/5; 5/12] END .........C=100, gamma=0.0001;, score=0.987 total time= 1.8min\n",
      "[CV 3/5; 8/12] START C=500, gamma=0.0001........................................\n",
      "[CV 5/5; 4/12] END ..........C=100, gamma=1e-05;, score=0.979 total time= 1.9min\n",
      "[CV 4/5; 8/12] START C=500, gamma=0.0001........................................\n",
      "[CV 3/5; 4/12] END ..........C=100, gamma=1e-05;, score=0.981 total time= 2.2min\n",
      "[CV 5/5; 8/12] START C=500, gamma=0.0001........................................\n",
      "[CV 5/5; 6/12] END ..........C=100, gamma=0.001;, score=0.988 total time= 1.0min\n",
      "[CV 1/5; 9/12] START C=500, gamma=0.001.........................................\n",
      "[CV 4/5; 6/12] END ..........C=100, gamma=0.001;, score=0.990 total time= 1.2min\n",
      "[CV 2/5; 9/12] START C=500, gamma=0.001.........................................\n",
      "[CV 4/5; 4/12] END ..........C=100, gamma=1e-05;, score=0.980 total time= 2.2min\n",
      "[CV 3/5; 9/12] START C=500, gamma=0.001.........................................\n",
      "[CV 5/5; 5/12] END .........C=100, gamma=0.0001;, score=0.987 total time= 1.7min\n",
      "[CV 4/5; 9/12] START C=500, gamma=0.001.........................................\n",
      "[CV 3/5; 5/12] END .........C=100, gamma=0.0001;, score=0.986 total time= 1.9min\n",
      "[CV 5/5; 9/12] START C=500, gamma=0.001.........................................\n",
      "[CV 4/5; 5/12] END .........C=100, gamma=0.0001;, score=0.987 total time= 1.9min\n",
      "[CV 1/5; 10/12] START C=1000, gamma=1e-05.......................................\n",
      "[CV 1/5; 9/12] END ..........C=500, gamma=0.001;, score=0.987 total time=  47.6s\n",
      "[CV 2/5; 10/12] START C=1000, gamma=1e-05.......................................\n",
      "[CV 2/5; 9/12] END ..........C=500, gamma=0.001;, score=0.987 total time=  47.9s\n",
      "[CV 3/5; 10/12] START C=1000, gamma=1e-05.......................................\n",
      "[CV 2/5; 7/12] END ..........C=500, gamma=1e-05;, score=0.981 total time= 1.9min\n",
      "[CV 4/5; 10/12] START C=1000, gamma=1e-05.......................................\n",
      "[CV 3/5; 9/12] END ..........C=500, gamma=0.001;, score=0.987 total time= 1.0min\n",
      "[CV 5/5; 10/12] START C=1000, gamma=1e-05.......................................\n",
      "[CV 4/5; 9/12] END ..........C=500, gamma=0.001;, score=0.987 total time=  53.0s\n",
      "[CV 1/5; 11/12] START C=1000, gamma=0.0001......................................\n",
      "[CV 5/5; 9/12] END ..........C=500, gamma=0.001;, score=0.986 total time=  52.5s\n",
      "[CV 2/5; 11/12] START C=1000, gamma=0.0001......................................\n",
      "[CV 1/5; 7/12] END ..........C=500, gamma=1e-05;, score=0.985 total time= 2.2min\n",
      "[CV 3/5; 11/12] START C=1000, gamma=0.0001......................................\n",
      "[CV 2/5; 8/12] END .........C=500, gamma=0.0001;, score=0.989 total time= 1.6min\n",
      "[CV 4/5; 11/12] START C=1000, gamma=0.0001......................................\n",
      "[CV 1/5; 8/12] END .........C=500, gamma=0.0001;, score=0.989 total time= 1.8min\n",
      "[CV 5/5; 11/12] START C=1000, gamma=0.0001......................................\n",
      "[CV 4/5; 7/12] END ..........C=500, gamma=1e-05;, score=0.982 total time= 2.1min\n",
      "[CV 1/5; 12/12] START C=1000, gamma=0.001.......................................\n",
      "[CV 3/5; 7/12] END ..........C=500, gamma=1e-05;, score=0.982 total time= 2.4min\n",
      "[CV 2/5; 12/12] START C=1000, gamma=0.001.......................................\n",
      "[CV 5/5; 7/12] END ..........C=500, gamma=1e-05;, score=0.983 total time= 2.2min\n",
      "[CV 3/5; 12/12] START C=1000, gamma=0.001.......................................\n",
      "[CV 5/5; 8/12] END .........C=500, gamma=0.0001;, score=0.989 total time= 1.9min\n",
      "[CV 4/5; 12/12] START C=1000, gamma=0.001.......................................\n",
      "[CV 3/5; 8/12] END .........C=500, gamma=0.0001;, score=0.987 total time= 2.1min\n",
      "[CV 5/5; 12/12] START C=1000, gamma=0.001.......................................\n",
      "[CV 4/5; 8/12] END .........C=500, gamma=0.0001;, score=0.988 total time= 2.2min\n",
      "[CV 2/5; 12/12] END ........C=1000, gamma=0.001;, score=0.986 total time=  39.3s\n",
      "[CV 1/5; 12/12] END ........C=1000, gamma=0.001;, score=0.987 total time=  43.5s\n",
      "[CV 1/5; 10/12] END ........C=1000, gamma=1e-05;, score=0.984 total time= 2.2min\n",
      "[CV 2/5; 10/12] END ........C=1000, gamma=1e-05;, score=0.982 total time= 1.8min\n",
      "[CV 4/5; 12/12] END ........C=1000, gamma=0.001;, score=0.986 total time=  48.1s\n",
      "[CV 2/5; 11/12] END .......C=1000, gamma=0.0001;, score=0.989 total time= 1.5min\n",
      "[CV 3/5; 12/12] END ........C=1000, gamma=0.001;, score=0.986 total time=  51.0s\n",
      "[CV 5/5; 12/12] END ........C=1000, gamma=0.001;, score=0.985 total time=  49.7s\n",
      "[CV 1/5; 11/12] END .......C=1000, gamma=0.0001;, score=0.990 total time= 1.6min\n",
      "[CV 5/5; 10/12] END ........C=1000, gamma=1e-05;, score=0.985 total time= 1.7min\n",
      "[CV 3/5; 10/12] END ........C=1000, gamma=1e-05;, score=0.984 total time= 2.0min\n",
      "[CV 5/5; 11/12] END .......C=1000, gamma=0.0001;, score=0.989 total time= 1.4min\n",
      "[CV 3/5; 11/12] END .......C=1000, gamma=0.0001;, score=0.987 total time= 1.5min\n",
      "[CV 4/5; 10/12] END ........C=1000, gamma=1e-05;, score=0.983 total time= 1.9min\n",
      "[CV 4/5; 11/12] END .......C=1000, gamma=0.0001;, score=0.989 total time= 1.5min\n",
      "CPU times: user 22.8 s, sys: 1.6 s, total: 24.4 s\n",
      "Wall time: 6min 28s\n",
      "{'C': 50, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "model_rbf = SVC(kernel='rbf')\n",
    "\n",
    "param_grid = {'C': [50, 100, 500, 1000],\n",
    "              'gamma': [1E-5, 0.0001, 0.001]}\n",
    "              #'class_weight': ['balanced', None]}\n",
    "# use -1 to use all the CPUs to perform the grid search.\n",
    "grid_rbf = GridSearchCV(model_rbf, param_grid, n_jobs=-1, verbose=20)\n",
    "\n",
    "sample_weight = [500 if yt == 'pos' else 10 for yt in ytrain]\n",
    "%time grid_rbf.fit(X_train_scaled, ytrain, sample_weight=sample_weight)\n",
    "print(grid_rbf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV 1/5; 1/4] START C=50........................................................\n",
      "[CV 4/5; 1/4] START C=50........................................................\n",
      "[CV 3/5; 1/4] START C=50........................................................\n",
      "[CV 2/5; 1/4] START C=50........................................................\n",
      "[CV 1/5; 2/4] START C=100.......................................................\n",
      "[CV 2/5; 2/4] START C=100.......................................................\n",
      "[CV 4/5; 2/4] START C=100.......................................................\n",
      "[CV 5/5; 2/4] START C=100.......................................................\n",
      "[CV 2/5; 3/4] START C=500.......................................................\n",
      "[CV 1/5; 3/4] START C=500.......................................................\n",
      "[CV 5/5; 1/4] START C=50........................................................\n",
      "[CV 3/5; 3/4] START C=500.......................................................\n",
      "[CV 3/5; 2/4] START C=100.......................................................\n",
      "[CV 5/5; 3/4] START C=500.......................................................\n",
      "[CV 1/5; 4/4] START C=1000......................................................\n",
      "[CV 4/5; 3/4] START C=500.......................................................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m/store/venv/da5401/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/store/venv/da5401/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/store/venv/da5401/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/store/venv/da5401/lib/python3.10/site-packages/sklearn/model_selection/_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    961\u001b[0m         )\n\u001b[1;32m    962\u001b[0m     )\n\u001b[0;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m     )\n",
      "File \u001b[0;32m/store/venv/da5401/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/store/venv/da5401/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/store/venv/da5401/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/store/venv/da5401/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m yt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m yt \u001b[38;5;129;01min\u001b[39;00m ytrain]\n\u001b[1;32m      8\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_lin.fit(X_train_scaled, ytrain, sample_weight=sample_weight)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgrid_lin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "model_lin = SVC(kernel='linear')\n",
    "\n",
    "param_grid_lin = {'C': [50, 100, 500, 1000]}\n",
    "# use -1 to use all the CPUs to perform the grid search.\n",
    "grid_lin = GridSearchCV(model_lin, param_grid_lin, n_jobs=-1, verbose=20)\n",
    "\n",
    "sample_weight = [500 if yt == 'pos' else 10 for yt in ytrain]\n",
    "%time grid_lin.fit(X_train_scaled, ytrain, sample_weight=sample_weight)\n",
    "print(grid_lin.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_rbf.best_estimator_\n",
    "\n",
    "model_svc = SVC(kernel='rbf', C=50, gamma=0.001).fit(X_train_scaled, ytrain, sample_weight=sample_weight)\n",
    "\n",
    "yfit = model_svc.predict(X_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg Data points: 44254\n",
      "Neg Support Points: 1019\n",
      "Pos Data points: 746\n",
      "Pos Support Points: 435\n"
     ]
    }
   ],
   "source": [
    "print(\"Neg Data points:\", np.sum(ytrain=='neg'))\n",
    "print(\"Neg Support Points:\", np.sum(ytrain[best_model.support_]=='neg'))\n",
    "print(\"Pos Data points:\", np.sum(ytrain=='pos'))\n",
    "print(\"Pos Support Points:\", np.sum(ytrain[best_model.support_]=='pos'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.99      1.00      0.99     14746\n",
      "         pos       0.71      0.60      0.65       254\n",
      "\n",
      "    accuracy                           0.99     15000\n",
      "   macro avg       0.85      0.80      0.82     15000\n",
      "weighted avg       0.99      0.99      0.99     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest, yfit, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "labels = [\"False\", 'True']\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
    "            cbar=True, cmap='Reds',\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.99      1.00      0.99     14746\n",
      "         pos       0.92      0.37      0.53       254\n",
      "\n",
      "    accuracy                           0.99     15000\n",
      "   macro avg       0.96      0.69      0.76     15000\n",
      "weighted avg       0.99      0.99      0.99     15000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[14738     8]\n",
      " [  159    95]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/store/venv/da5401/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.99      1.00      0.99     14746\n",
      "         pos       0.88      0.36      0.51       254\n",
      "\n",
      "    accuracy                           0.99     15000\n",
      "   macro avg       0.93      0.68      0.75     15000\n",
      "weighted avg       0.99      0.99      0.99     15000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[14733    13]\n",
      " [  163    91]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=10000, class_weight={'pos':10, 'neg':500}).fit(X_train_scaled, ytrain)\n",
    "yhat_logreg = logreg.predict(X_test_scaled)\n",
    "print(classification_report(ytest, yhat_logreg, zero_division=0))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(ytest, yhat_logreg))\n",
    "\n",
    "logreg = LogisticRegression(max_iter=10000, class_weight={'pos':10, 'neg':500}).fit(Xtrain, ytrain)\n",
    "yhat_logreg = logreg.predict(Xtest)\n",
    "print(classification_report(ytest, yhat_logreg, zero_division=0))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(ytest, yhat_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos Training data: (746, 170)\n",
      "Neg Undersampled Training data: (5000, 170)\n",
      "Positive Support points: 435\n",
      "Negative Support points: 1019\n",
      "Stacked Training data: (7200, 170)\n",
      "Resampled Train Data: pos =  1181\n",
      "Resampled Train Data: neg =  6019\n",
      "\n",
      "Testing Classification Performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.99      0.99      0.99     14746\n",
      "         pos       0.61      0.63      0.62       254\n",
      "\n",
      "    accuracy                           0.99     15000\n",
      "   macro avg       0.80      0.81      0.81     15000\n",
      "weighted avg       0.99      0.99      0.99     15000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[14645   101]\n",
      " [   95   159]]\n"
     ]
    }
   ],
   "source": [
    "# get the positive points\n",
    "ytrain_pos = ytrain[ytrain=='pos']\n",
    "xtrain_pos = X_train_scaled[ytrain=='pos']\n",
    "print(\"Pos Training data:\", xtrain_pos.shape)\n",
    "\n",
    "# get the negative from the dataset.\n",
    "ytrain_neg = ytrain[ytrain=='neg']\n",
    "\n",
    "# under sample the negative points\n",
    "ids = np.arange(0, ytrain_neg.shape[0])\n",
    "sample_ids = np.random.choice(ids, 5000, replace=False)\n",
    "\n",
    "ytrain_neg = ytrain_neg[sample_ids]\n",
    "xtrain_neg = X_train_scaled[sample_ids]\n",
    "print(\"Neg Undersampled Training data:\", xtrain_neg.shape)\n",
    "\n",
    "print(\"Positive Support points:\", np.sum(ytrain[best_model.support_]=='pos'))\n",
    "print(\"Negative Support points:\", np.sum(ytrain[best_model.support_]=='neg'))\n",
    "\n",
    "# add the positives, undersampled negatives, and support points\n",
    "X_stacked = np.concatenate((X_train_scaled[best_model.support_], xtrain_pos, xtrain_neg), axis=0)\n",
    "y_stacked = np.concatenate((ytrain[best_model.support_], ytrain_pos, ytrain_neg), axis=0)\n",
    "print(\"Stacked Training data:\", X_stacked.shape)\n",
    "\n",
    "print(\"Resampled Train Data: pos = \", np.sum(y_stacked=='pos'))\n",
    "print(\"Resampled Train Data: neg = \", np.sum(y_stacked=='neg'))\n",
    "\n",
    "logreg_s = LogisticRegression(max_iter=10000, class_weight={'pos':10, 'neg':20}).fit(X_stacked, y_stacked)\n",
    "yhat_logreg = logreg_s.predict(X_test_scaled)\n",
    "print(\"\\nTesting Classification Performance:\\n\", classification_report(ytest, yhat_logreg, zero_division=0))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(ytest, yhat_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.99      1.00      0.99     14746\n",
      "         pos       0.68      0.31      0.42       254\n",
      "\n",
      "    accuracy                           0.99     15000\n",
      "   macro avg       0.83      0.65      0.71     15000\n",
      "weighted avg       0.98      0.99      0.98     15000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[14709    37]\n",
      " [  176    78]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.99      1.00      0.99     14746\n",
      "         pos       0.68      0.31      0.42       254\n",
      "\n",
      "    accuracy                           0.99     15000\n",
      "   macro avg       0.83      0.65      0.71     15000\n",
      "weighted avg       0.98      0.99      0.98     15000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[14709    37]\n",
      " [  176    78]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_tree = DecisionTreeClassifier(max_depth=2, class_weight={'pos':5, 'neg':3}, random_state=42).fit(X_train_scaled, ytrain)\n",
    "yhat_tree = model_tree.predict(X_test_scaled)\n",
    "print(classification_report(ytest, yhat_tree, zero_division=0))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(ytest, yhat_tree))\n",
    "\n",
    "model_tree = DecisionTreeClassifier(max_depth=2, class_weight={'pos':5, 'neg':3}, random_state=42).fit(Xtrain, ytrain)\n",
    "yhat_tree = model_tree.predict(Xtest)\n",
    "print(classification_report(ytest, yhat_tree, zero_division=0))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(ytest, yhat_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da5401",
   "language": "python",
   "name": "da5401"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
